{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3314048",
   "metadata": {},
   "source": [
    "# Make more Part 4: Diving deep into back prop\n",
    "- Implementing back prop by hand from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9561c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed02f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../data/names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0e1aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build vocab of characters\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c450cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the dataset\n",
    "def build_dataset(words):\n",
    "    block_size = 3\n",
    "    X,Y = [],[]\n",
    "    for w in words:\n",
    "        context = [0]*block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4801c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val and test splits:\n",
    "import random\n",
    "random.seed(42)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xval, Yval = build_dataset(words[n1:n2])\n",
    "Xtest, Ytest = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f675c45",
   "metadata": {},
   "source": [
    "### Introducing a gradient checker function\n",
    "- Checks the gradients collected with our implementation of back prop with the one\n",
    "- We use Bessel's correction: divide by n-1 instead of n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e9dd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_comparator(s:str,dt:torch.Tensor, t:torch.Tensor):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        s (str): Label\n",
    "        dt (torch.Tensor): Pytorch gradient\n",
    "        t (torch.Tensor): Our gradient\n",
    "    \"\"\"\n",
    "    exact_match = torch.all(dt==t.grad).item()\n",
    "    approx_match = torch.allclose(dt, t.grad)\n",
    "    max_diff = torch.max(torch.abs(dt-t.grad)).item()\n",
    "    print(f'{s:15s} | exact_match: {str(exact_match):5s} | approx_match: {str(approx_match):5s} | max_diff: {max_diff}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fabccf",
   "metadata": {},
   "source": [
    "### Initialising Network\n",
    "- We will use some non standard ways to initialise network just to make sure we have gradients for all layers and can verify them.\n",
    "- Setting biases to random values instead of zeros. Because sometimes initialising with zeros could mask incorrect implementation of back prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "28a9886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 10\n",
    "n_hidden = 64\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "fan_in = n_embed*block_size\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embed), generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((fan_in, n_hidden), generator=g) * (5/3)*(fan_in**-0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# batch norm params:\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1,n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb4aa9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =32\n",
    "n = batch_size\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e1bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4808, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logits_max = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logits_max # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logits_max, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ebe75025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([1, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bndiff.shape,bnmeani.shape, hprebn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977e610",
   "metadata": {},
   "source": [
    "# Backpropagating through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ff204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs       | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "probs           | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "counts_sum_inv  | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "counts_sum      | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "counts          | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "norm_logits     | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "logits_max      | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "logits          | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "h               | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "W2              | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "b2              | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "hpreact         | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "bngain          | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "bnraw           | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "bnbias          | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "bnvar_inv       | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "bnvar           | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "bndiff2         | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "bndiff          | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "bnmeani         | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "hprebn          | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "embcat          | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "W1              | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "b1              | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "emb             | exact_match: True  | approx_match: True  | max_diff: 0.0\n",
      "C               | exact_match: True  | approx_match: True  | max_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculating gradient of log prob\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[torch.arange(n), Yb] = -1.0/n\n",
    "## Compare gradients:\n",
    "gradient_comparator('log_probs',dlogprobs, logprobs)\n",
    "\n",
    "# Step 2: Calculating gradient of probs:\n",
    "dprobs = (1/probs) * dlogprobs # chain rule\n",
    "## Compare gradients:\n",
    "gradient_comparator('probs',dprobs, probs)\n",
    "\n",
    "# Step 3: Calculating gradient of Inverse of counts sum:\n",
    "dcounts_sum_inv = (counts * dprobs).sum(dim=1, keepdim=True) # sum because counts_sum_inv is broadcasted to counts so we need to sum it's gradients\n",
    "## Compare gradients:\n",
    "gradient_comparator('counts_sum_inv',dcounts_sum_inv, counts_sum_inv)\n",
    "\n",
    "# Step 4: Calculating gradient of counts sum:\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "\n",
    "# Step 5: Calculating gradient of counts_sum:\n",
    "dcounts_sum = (-counts_sum**-2)* dcounts_sum_inv\n",
    "gradient_comparator('counts_sum', dcounts_sum, counts_sum)\n",
    "\n",
    "# Step 6: Calculating gradient of counts:\n",
    "dcounts += torch.ones_like(counts)* dcounts_sum # combine gradients from both branches of dcounts\n",
    "## Compare gradients:\n",
    "gradient_comparator('counts', dcounts, counts)\n",
    "\n",
    "# Step 7: Calculating gradient of norm logits:\n",
    "dnormlogits = dcounts * norm_logits.exp()\n",
    "gradient_comparator('norm_logits', dnormlogits, norm_logits)\n",
    "\n",
    "# Step 8: Calculating gradient for logits:\n",
    "dlogits = dnormlogits.clone() \n",
    "\n",
    "# Step 9: Calculating gradient of logit_max:\n",
    "dlogits_max = (-dnormlogits).sum(dim=1, keepdim=True)\n",
    "gradient_comparator('logits_max', dlogits_max, logits_max)\n",
    "\n",
    "# Step 10: Calculating gradient for logits:\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1])*dlogits_max\n",
    "gradient_comparator('logits', dlogits, logits)\n",
    "\n",
    "# Step 11: Calculating gradient of h, W2 and b2\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "gradient_comparator('h', dh, h)\n",
    "gradient_comparator('W2', dW2, W2)\n",
    "gradient_comparator('b2', db2, b2)\n",
    "\n",
    "# Step 12: Calculating gradient of hpreact\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "gradient_comparator('hpreact', dhpreact, hpreact)\n",
    "\n",
    "# Step 13: Calculating gradient of bngain, bnraw, bnbias\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "gradient_comparator('bngain', dbngain, bngain)\n",
    "gradient_comparator('bnraw', dbnraw, bnraw)\n",
    "gradient_comparator('bnbias', dbnbias, bnbias)\n",
    "\n",
    "# Step 14 Calculating gradient of bndiff and bnvar_inv\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "gradient_comparator('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "\n",
    "# Step 15: Calculating gradient of bnvar:\n",
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "gradient_comparator('bnvar', dbnvar, bnvar)\n",
    "\n",
    "# Step 16: Calculate gradient of bndiff2:\n",
    "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "gradient_comparator('bndiff2', dbndiff2, bndiff2)\n",
    "\n",
    "## Second of bndiff:\n",
    "dbndiff += (2*bndiff) * dbndiff2\n",
    "gradient_comparator('bndiff', dbndiff, bndiff)\n",
    "\n",
    "# Step 17: Calculate gradient of hprebn and bnmeani:\n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "gradient_comparator('bnmeani', dbnmeani, bnmeani)\n",
    "\n",
    "## Second part of dhprebn:\n",
    "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "gradient_comparator('hprebn', dhprebn, hprebn)\n",
    "\n",
    "# Step 18: Calculate gradient of embcat, W1 and b1:\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "gradient_comparator('embcat', dembcat, embcat)\n",
    "gradient_comparator('W1', dW1, W1)\n",
    "gradient_comparator('b1', db1, b1)\n",
    "\n",
    "# Step 19: Calculate gradient of emb:\n",
    "demb = dembcat.view(emb.shape)\n",
    "gradient_comparator('emb', demb, emb)\n",
    "\n",
    "# Step 20: Calculate gradient of C:\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[i,j]\n",
    "        dC[ix] += demb[i,j]\n",
    "gradient_comparator('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b1f4f",
   "metadata": {},
   "source": [
    "# Calculating simplified gradients for logits:\n",
    "- Instead of calculating gradients for each and every step of back prop, we can simplify it by calculating gradient of loss wrt logits\n",
    "- We can use softmax to calculate this\n",
    "- The gradients boils down to 2 calculations:\n",
    "    - When y != true label, Softmax is the gradient\n",
    "    - When y==True label, Softmax-1 is the  gradient\n",
    "    - And we need to scale the gradient by n because we calculate avg of softmax as our loss\n",
    "- The idea is: dlogits represents the amount by which we push probabilities up for correct characters and down for incorrect characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c2eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact_match: False | approx_match: True  | max_diff: 4.6566128730773926e-09\n"
     ]
    }
   ],
   "source": [
    "# simplified gradients for logits:\n",
    "dlogits = F.softmax(logits, dim=1)\n",
    "dlogits[range(n), Yb] -=1\n",
    "dlogits /= n\n",
    "gradient_comparator('logits', dlogits, logits) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7f4d7",
   "metadata": {},
   "source": [
    "## Visualizing dlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aee72b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x135040e10>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e605b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhprebn         | exact_match: False | approx_match: True  | max_diff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "## MOdifying batch norm to fit in a single step\n",
    "## Using calculations by hand to calculate gradients \n",
    "dhprebn = bngain * bnvar_inv/n * (n * dhpreact -dhpreact.sum(0) - n/(n-1)*bnraw * (dhpreact*bnraw).sum(0))\n",
    "gradient_comparator('dhprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2547a9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200000 loss: 3.2878\n",
      "10000 / 200000 loss: 2.1231\n",
      "20000 / 200000 loss: 2.3439\n",
      "30000 / 200000 loss: 2.1008\n",
      "40000 / 200000 loss: 2.0448\n",
      "50000 / 200000 loss: 2.3835\n",
      "60000 / 200000 loss: 1.9799\n",
      "70000 / 200000 loss: 1.9750\n",
      "80000 / 200000 loss: 2.1440\n",
      "90000 / 200000 loss: 2.3039\n",
      "100000 / 200000 loss: 2.1721\n",
      "110000 / 200000 loss: 2.1609\n",
      "120000 / 200000 loss: 1.8855\n",
      "130000 / 200000 loss: 2.2752\n",
      "140000 / 200000 loss: 1.9743\n",
      "150000 / 200000 loss: 2.6401\n",
      "160000 / 200000 loss: 2.2051\n",
      "170000 / 200000 loss: 2.2805\n",
      "180000 / 200000 loss: 1.9105\n",
      "190000 / 200000 loss: 1.7767\n"
     ]
    }
   ],
   "source": [
    "## Putting everything together\n",
    "\n",
    "## Reinitializing the model\n",
    "n_embed = 10\n",
    "n_hidden = 64\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "fan_in = n_embed*block_size\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embed), generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((fan_in, n_hidden), generator=g) * (5/3)*(fan_in**-0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# batch norm params:\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1,n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "batch_size =32\n",
    "max_steps = 200000\n",
    "losses = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    # Linear layer 1\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "    bndiff = hprebn - bnmeani\n",
    "    bndiff2 = bndiff**2\n",
    "    bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = bndiff * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    \n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    # Linear layer 2\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    # cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "    # PyTorch backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    #loss.backward()\n",
    "    \n",
    "    # Manual backprop\n",
    "    ## dlogits\n",
    "    dlogits = F.softmax(logits, dim=1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "\n",
    "    ## 2nd layer backprop\n",
    "    ## dh\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "\n",
    "    ## tanh backprop\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    \n",
    "    ## batchnorm backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = (dhpreact).sum(0, keepdim=True)\n",
    "    dhprebn = bngain * bnvar_inv/n * (n * dhpreact -dhpreact.sum(0) - n/(n-1)*bnraw * (dhpreact*bnraw).sum(0))\n",
    "\n",
    "    ## 1st layer backprop\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "\n",
    "    ## Embedding backprop\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "        for j in range(Xb.shape[1]):\n",
    "            ix = Xb[k,j]\n",
    "            dC[ix] += demb[k,j]\n",
    "            \n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    \n",
    "    # lr update\n",
    "    lr = 0.1 if i<100000 else 0.01\n",
    "    for p, grad in zip(parameters, grads):\n",
    "        p.data += -lr * grad\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i} / {max_steps} loss: {loss.item():.4f}')   \n",
    "    losses.append(loss.log10().item())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cb04933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate batch norm params at the end of training\n",
    "with torch.no_grad():\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    \n",
    "    # mean and std over entire dataset\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnvar = hpreact.var(0, keepdim=True, unbiased = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "25470370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('train', 2.486947536468506), ('val', 2.7306370735168457))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad() # When we want to disable gradient tracking\n",
    "def split_loss_bn(split:str) -> tuple[str, float]:\n",
    "    X,y = {\n",
    "        'train':(Xtr, Ytr),\n",
    "        'val':(Xval, Yval),\n",
    "        'test':(Xtest, Ytest)\n",
    "    }[split]\n",
    "    emb = C[X]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmeani) + (bnvar + 1e-5)**-0.5 * bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return (split, loss.item())\n",
    "\n",
    "split_loss_bn('train'), split_loss_bn('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aa289085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chruzhfani.\n",
      "heva.\n",
      "khmrristetyk.\n",
      "kayssaetrathtefaphirha.\n",
      "treei.\n",
      "jerosett.\n",
      "maiheda.\n",
      "lett.\n",
      "dhlis.\n",
      "kayzquinn.\n",
      "suovl.\n",
      "amivubrionnella.\n",
      "jaristifradipitrathed.\n",
      "ezo.\n",
      "akettefathraquhpasmouk.\n",
      "tksymrismbwion.\n",
      "zuloj.\n",
      "mikah.\n",
      "fren.\n",
      "kpusadhtuv.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
    "        embcat = emb.view(emb.shape[0], -1)\n",
    "        hpreact = embcat @ W1 + b1\n",
    "        hpreact = bngain * (hpreact - bnmeani) + (bnvar + 1e-5)**-0.5 * bnbias\n",
    "        h = torch.tanh(hpreact)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4122af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "makemore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
